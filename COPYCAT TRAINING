OK, now for the third part, we're going to do the actual copycat training. So we have two postage stamps. One for the reference is going to be the target data set that's going to be aligned. The other postage stamp is going to be the source data set. So for the target data set, most of the time when we are using a video reference, or a television reference, or some sort of reference, it may come with a lot of dust, dirt, compression, a lot of elements that may introduce artifacts to the image. So what I usually do is to do some sort of filtering. That's why I use a median. So a median filter, I think, is the best way to filter this kind of stuff. Because the blur, unless you're using a really specific kind of blur, doesn't really achieve what I want to do here. So we do a median. Now the median, I'm doing the size 10. But it really depends on the quality of the reference. Sometimes the reference may need a lot less. But sometimes you may need a lot more. But in this case, 10, I think, was a good enough solution for this particular case. So it's something that you need to be aware of. And this highly depends on the quality of the reference. Sometimes you may only need to do one or two points, maybe three or four points. So that's something to take in consideration. For the postage stamp of the source data set, we're going to apply the same crop, the same linked crop or the same clone crop that we used in the alignment phase is going to be applied here. And then to both paths, we're going to use a color space. That color space is going to allow us. Because right now, we're working in RGB. So instead of working in RGB, we need to change to YUV. Why? Because we need to separate chroma from luma. So we're going to do a color space transform for both paths. And it's going to be linear to YUV. So we're going from a linear workflow to YCVCR or YUV. Then once we are in YUV, we're going to do a shuffle node. That shuffle node is going to do something really specific. It's going to take, since remember, we are not working in RGB anymore. We are working in YUV. So in YUV, red becomes luma. Green becomes U or CV. And blue becomes CR or B. And since we want the moment in the reference, in the reference, we only want the chroma information. We don't want the luma information because the luma information, since this is a video or a telecine or whatever, is usually really low. And we don't want to mess with that. So what we're going to do is extract the luma from the source and put it with the chroma of the reference. So basically, we are only keeping the CV and the CR of the reference, but using the luma of the source to kind of create this new version, this ground truth that we want to achieve. So once we do that, and once we have extracted the luma out of the source, we go back to RGB. So we do another color space transform that goes from YCVCR to linear. Then basically, at this point, we are kind of ready. Our ground truth and our source should be exactly the same. But we need to do some cleanup, especially to avoid any type of artifacts. So we're going to do a grade node. This grade node, exactly what is the only thing that this grade node is doing is a black clamp and a white clamp to avoid any type of sub-0 values or above 1 values. So basically, we're doing a black clamp and a white clamp. Then we are going to do a remove channel operation. And in this remove channel operation, we are removing the alpha channel. Since we only want to train RGB, and to be honest, CopyCat works better when you only train the RGB channels, then we need to remove the extra channel that's going to be the alpha channel. So we do the same for both. And to avoid any kind of issue with the bounding box, then we set up to copy the bounding box from the reference to the source. And it's kind of a quality control step as well. Then out of this result, then we can have a true ground truth and a true input. So at this point, the only difference, since we have extracted the luma from the source and used the chroma from the reference, the only difference between the ground truth and the input should be color. Everything else should be exactly the same. They should have the same information, the same resolution. Everything else should be exactly the same. The only difference is that one should have the original colors of the reference, on the other hand, should have the source colors that are faded. So once we have that, then we are ready to do the training. So for the actual training, I have some specific settings that I used in the past that work really well for me, especially when I'm doing this shot by shot. So obviously, we need to use a GPU. In the case of Mac, you can only do Apple Silicon. Well, to a degree, you can use some of the old Navi cards. If you're using a Mac Pro 2019, it should work as well. I think the performance is better tuned to use Apple Silicon. Now, even if you have the fanciest Apple Silicon, it's going to fail in comparison to a middle tier NVIDIA GPU. So NVIDIA is still going to be king in this. So in my particular case, in my lab back at home, I have four 3090s. So I use that. And we set the directory to whatever we want to set. Usually, I create different folders for all the different shots. So I have a directory for each of the training. Then the number of epochs, I usually set the number of epochs to the number of. Usually for me, the number of epochs is going to be exactly the same as the number of the batch size multiplied by 10,000. So if my batch size is going to be 3, then my epochs are going to be 30,000, something like that. Now, this changes when the crop size is when I have to change the crop size. Sometimes when I have to use, especially when I'm removing subtitles, I need to use a crop size that is not 512. I may need to use a crop size that is 256. So in those cases, I usually increase the numbers of epochs to be the double of the batch size. So in this case, instead of 30,000, it will be 60,000. Stop. 60,000, yes, 60,000. So that's basically it. The initial weights in the advanced option is going to be none. The model size, I always left it at medium. I may do in the future a trial using a large model. At this point, medium is going good. The batch size, I never used auto because I had issues in the past with training that didn't come out actually as I wanted because I usually jump between different machines. And sometimes there's machines that have different GPUs and the different GPUs usually have different VRAMs and different bandwidths. So for me, the batch size set to 3 makes me have a more consistent result between different machines. This might not be the case for some of the people working with this, but for me, this works. The crop size, I always set it to the maximum available. So for most times, it's going to be 512. In some cases, like I told you, it may be 256, depending on the size of the framing. The checkpoint interval, I usually let it at 10,000. Since I'm usually doing between 40,000 and 80,000 steps, for me, a checkpoint of 10,000 is OK because I always have a good place to start again from if I need to start again. The contact sheet, I always let it at 100 because I prefer having a clear, concise. 
In the case of the checkpoint interval, I usually set it at 10,000 because I usually train for around 40,000 steps, sometimes 80,000, so 10,000 is a good way to start again if I need to start again, or if something happens in the training I can easily start from a good point. In the case of the contact sheet, I usually let it at 100 because I need to be on top of what is happening in the training, so for me 100 is a good way to know exactly what is happening at all time, because if I set it to 1,000 or to a bigger number, it might take me a while to properly guess what is happening, so it's not going to be useful. And obviously I'm going to use multi-resolution training because having the training go through different resolutions is going to give me a better performance, and it usually speeds up the training by three times, so I like it. Because in the end, I remember what they say, it usually does a first pass, and that first pass is like low frequencies, then medium frequencies, then high frequencies, and when it gets to the high-resolution training, it's usually just finishing the details, so it doesn't need to go over. But sometimes you may see a better result by disabling multi-resolution training, but in my case, in the case especially of color, I think multi-resolution training works just fine. Might need to try that with spatial resolution, might change that. Obviously, when we have everything set up, then we just start the training, and we check the progress here on the progress tab. Another extra thing that I do when I do the copycat training is that I set a preview. So a preview is usually a frame hold, I usually set the source again in some way, in some place, and then I set a frame hold to some place that I haven't been trained. So let's say I trained the first and the last frame, so I'm training frames 1, 8, 6, and 4. So I might set the preview to frame 5 in order to see how is the model working in other frames that are not the ones being trained on. So in that case, I can quickly check what's happening. Just to know, just to have a good enough preview to see what's happening with the model. Now we go to the final step, that is the inference render. And the inference render is quite straightforward. We just set the postage stamp, it's basically the source sequence. We set another crop. In this step, the crop that we are doing, this crop is just for us to remove any other element that is not imaged. So in case we have optical tracks of sound, or maybe the borders of the frame, we need to eliminate that to only leave us with the actual image. Then we're going to apply the inference, that's going to be the result of the training, we're going to apply that inference. Now since I'm using the non-commercial version of Nuke, in order for me to render the results, I need to reformat this to 1920x1080. This is a non-issue when you're using Nuke Indie, or the complete NukeX platform, because you can have access to other resolutions. So this step of rendering 1080 is only for the NukeX non-commercial version. Then I render the sequence as an EXR. In this case, the EXR that I'm rendering is going to be an ACES2065-1 rendered in EXR with DWAA compression at 16 bit half floating point. Then, when I render the file, I import the rendered file, and I check the consistency to see if the render has gone completely, and if everything is good.